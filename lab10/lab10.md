# 1
- 1a. we don't know (the reward function and) transition model in this case --> we can formulate reward function ourselves
- 2a. select b
- 2b. select b since not 
- 2c. same as 2b
- 3a. 

# 2
- 1a. have not got any reward, no update
- 1b. first time the agent got to goal state and took action
- 1c. as one state's q-value was updated, the q-values of the states that potentially leads to the initial reward state got also updated
- 1d. 19
- 1e. all pointing to the goal state, closer the brighter
- 1f. less iteration since more exploration
- 2a. change is happening more quickly since q-values are almost constantly updated; you'll find the goal state sooner, thus will find optimal path sooner

# 3
- 2a. epsilon = 1 is surprisingly good

# 4
- a. (s, b) = , (s, n) = , (ms, b) = , 
- b. forward when not bumping, backward when bumping
- 
